{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langgraph\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "import os\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any\n",
    "from utils.gemini_service import GeminiModel, GeminiJsonEngine, GeminiSimpleChatEngine\n",
    "from utils.logger import LOGGER\n",
    "import time\n",
    "import json\n",
    "\n",
    "import hashlib\n",
    "from sqlalchemy import create_engine, Column, String, Table, MetaData\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.sql import text\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/debasmitroy/Desktop/programming/gemini-agent-assist/key.json\"\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"hackathon0-project\"\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pydantic Base Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a financial query involving placeholders for key financial attributes: \n",
    "    <BUIS>, <DATE>, <NET>, <FACTOR>, <PROF_LOSS>, <CUR>, <PF>, and <DSK>.  \n",
    "\n",
    "    Example Queries:  \n",
    "    - What are the <FACTOR>s that contributed the highest <NET> profit/loss?  \n",
    "    - Which <CUR> currencies are driving the top-performing portfolios <PF>?  \n",
    "    \"\"\"\n",
    "    query: str = Field(..., title=\"Financial query using placeholders <BUIS>, <DATE>, <NET>, <FACTOR>, <PROF_LOSS>, <CUR>, <PF>, and <DSK>.\")\n",
    "\n",
    "class FinancialQueries(BaseModel):\n",
    "    \"\"\"\n",
    "    A collection of structured queries designed to generate financial summaries.  \n",
    "    Each query should use placeholders (<FIELD>) instead of actual values.  \n",
    "    \"\"\"\n",
    "    queries: List[Query] = Field(..., title=\"List of financial queries using placeholders <FIELD> instead of actual values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQLScript(BaseModel):\n",
    "    \"\"\"\n",
    "    SQL Script to query data from the given table. You have to use this tool to generate the SQL script.\n",
    "    \"\"\"\n",
    "    sql_script: str = Field(..., title=\"SQL Script to query data from the given table.\")\n",
    "    columns: List[str] = Field(..., title=\"Which columns are being projected in the SQL script.\")\n",
    "    description: str = Field(..., title=\"What does the SQL script do in Finance Analyst's perspective\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    state: str\n",
    "    model: Dict[str, Any]\n",
    "    results: Dict[str, Any]\n",
    "    cache_location: Dict[str, Any]\n",
    "    cache_flag: Dict[str, Any]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(path: str) -> Dict[str, Any]:\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    with open(path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def save_json_data(path: str, data: Dict[str, Any]):\n",
    "    # Create the directory if it doesn't exist\n",
    "    directory = os.path.dirname(path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    with open(path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def load_cached_results(state):\n",
    "    # Load the data from cache if the cache flag is set to True\n",
    "    if state['cache_flag'][state['state']]:\n",
    "        cached_result = load_json_data(state['cache_location'][state['state']])\n",
    "        if cached_result:\n",
    "            state['results'][state['state']] = cached_result['result']\n",
    "            LOGGER.info(f\"State: {state['state']} | Loaded cached data and skipping the model, {len(state['results'][state['state']])} old result found\")\n",
    "            return state\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InMemoryDB:\n",
    "    def __init__(self):\n",
    "        self.engine = create_engine('sqlite:///:memory:', echo=False)\n",
    "        self.metadata = MetaData()\n",
    "        self.Session = sessionmaker(bind=self.engine)\n",
    "        self.session = self.Session()\n",
    "\n",
    "    def create_table(self, table_name, columns):\n",
    "        \"\"\"Creates a table dynamically with a SHA-256 hash primary key to prevent duplicates.\"\"\"\n",
    "        table = Table(\n",
    "            table_name, self.metadata,\n",
    "            Column(\"id\", String, primary_key=True),  # Primary key hash column\n",
    "            *[Column(col, String) for col in columns],\n",
    "        )\n",
    "        table.create(self.engine)\n",
    "\n",
    "    def generate_hash(self, data):\n",
    "        \"\"\"Generates a SHA-256 hash over the string representation of a row.\"\"\"\n",
    "        row_string = str(sorted(data.items()))  # Ensure consistent ordering\n",
    "        return hashlib.sha256(row_string.encode()).hexdigest()\n",
    "\n",
    "    def insert_data(self, table_name, data):\n",
    "        \"\"\"Inserts a row into the table using parameterized queries and avoids duplicates.\"\"\"\n",
    "        data[\"id\"] = self.generate_hash(data)  # Add hash key to data\n",
    "        placeholders = \", \".join([f\":{key}\" for key in data.keys()])\n",
    "        query = text(f\"\"\"\n",
    "            INSERT INTO {table_name} ({', '.join(data.keys())})\n",
    "            VALUES ({placeholders})\n",
    "            ON CONFLICT(id) DO NOTHING\n",
    "        \"\"\")\n",
    "        self.session.execute(query, data)\n",
    "        self.session.commit()\n",
    "\n",
    "    def query_data(self, query):\n",
    "        \"\"\"Executes a SELECT query and returns results with column names.\"\"\"\n",
    "        result = self.session.execute(text(query))\n",
    "        columns = result.keys()  # Get column names\n",
    "        data = result.fetchall()  # Get data rows\n",
    "\n",
    "        # Convert data to list of list from list of tuples\n",
    "        data = [list(row) for row in data]\n",
    "        return list(columns), data  # Return both columns and data\n",
    "\n",
    "    def __del__(self):\n",
    "        self.session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_data_inmemory_db(rule_based_title_comment_data):\n",
    "    # Initialize DB and create table\n",
    "    columns = list(rule_based_title_comment_data[0].keys())\n",
    "    columns.remove(\"id\") if \"id\" in columns else None  # Ensure id isn't duplicated\n",
    "    inmemory_db = InMemoryDB()\n",
    "    inmemory_db.create_table(\"title_data\", columns)\n",
    "\n",
    "    # Insert data\n",
    "    for data in rule_based_title_comment_data:\n",
    "        inmemory_db.insert_data(\"title_data\", data)\n",
    "\n",
    "    return inmemory_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE_DATA_INMEM_DB = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_agent(state: AgentState):\n",
    "    LOGGER.info(\"Starting the agent-assist\")\n",
    "    state['state'] = 'start'\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_agent(state: AgentState):\n",
    "    LOGGER.info(\"Ending the agent-assist\")\n",
    "    state['state'] = 'end'\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_old_summary_agent(state: AgentState):\n",
    "    state['state'] = 'refine_old_summaries'\n",
    "    LOGGER.info(f\"State: {state['state']} | Initializing the agent to refine old summaries\")\n",
    "\n",
    "    # Load the data from cache if the cache flag is set to True\n",
    "    cached_result = load_cached_results(state)\n",
    "    if cached_result:\n",
    "        return state\n",
    "    \n",
    "    # Initialize the model\n",
    "    gemini_simple_chat_engine = GeminiSimpleChatEngine(model_name=state['model']['model_name'], \n",
    "                                                   temperature=state['model']['temperature'],\n",
    "                                                   max_output_tokens=state['model']['max_output_tokens'],\n",
    "                                                   systemInstructions=\"You are an expert financial bot. You will be given a financial report and you need to refine the report. Keep everything in a single large paragraph. Dont use any markdown or bullet points. \",\n",
    "                                                   max_retries=state['model']['max_retries'],\n",
    "                                                   wait_time=state['model']['wait_time'])\n",
    "\n",
    "    # Old summaries from sample_summarized_pnl_commentaries (Note: This is a sample data not cached, admin will provide the data)\n",
    "    sample_summarized_pnl_commentaries = load_json_data(state['cache_location']['sample_summarized_pnl_commentaries'])\n",
    "    LOGGER.info(f\"State: {state['state']} | Loaded the sample data, {len(sample_summarized_pnl_commentaries)} old summaries found\")\n",
    "    \n",
    "    # Refine the old summaries\n",
    "    result = []\n",
    "\n",
    "    for summary in sample_summarized_pnl_commentaries:\n",
    "        _refinement_prompt = [\n",
    "            f\"Given financial report: {summary}\",\n",
    "            f\"Please refine the financial report in a more readable and meangingful way without losing any important information and entitites and technical/financial terms. Dont unnecessarily change the meaning of the report and dont increase the length of the report. \"\n",
    "        ]\n",
    "        refined_summary = gemini_simple_chat_engine(_refinement_prompt)\n",
    "        result.append(refined_summary)\n",
    "        LOGGER.info(f\"State: {state['state']} | Summary refined , {summary[:30]}... to {refined_summary[:30]}...\")\n",
    "\n",
    "    # Save the result to state var and set the cache flag to True\n",
    "    state['results'][state['state']] = result\n",
    "    state['cache_flag'][state['state']] = True\n",
    "\n",
    "    # Save the result to cache with the state name and {result} key\n",
    "    save_json_data(state['cache_location'][state['state']], {\"result\":state['results'][state['state']]})\n",
    "    \n",
    "    LOGGER.info(f\"State: {state['state']} | Refinement of old summaries completed, saved the result to cache and set the cache flag to True\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subj_query_agent(state: AgentState):\n",
    "    state['state'] = 'subj_query_generation'\n",
    "    LOGGER.info(f\"State: {state['state']} | Initializing the agent to refine old summaries\")\n",
    "\n",
    "    # Load the data from cache if the cache flag is set to True\n",
    "    cached_result = load_cached_results(state)\n",
    "    if cached_result:\n",
    "        return state\n",
    "    \n",
    "    # Initialize the model\n",
    "    fin_qry_engine =  GeminiJsonEngine(\n",
    "                                    model_name=state['model']['model_name'],\n",
    "                                    basemodel=FinancialQueries,\n",
    "                                    temperature=state['model']['temperature'],\n",
    "                                    max_output_tokens=state['model']['max_output_tokens'],\n",
    "                                    systemInstructions=None,\n",
    "                                    max_retries=state['model']['max_retries'],\n",
    "                                    wait_time=state['model']['wait_time'])\n",
    "\n",
    "    # Outputs from the previous state\n",
    "    refined_sample_summarized_pnl_commentaries = state['results']['refine_old_summaries']\n",
    "\n",
    "    # Prompt\n",
    "    title_comment_template = \"For Buisness <BUIS>, on  <DATE>, driven by <NET>$  <FACTOR> <PROF_LOSS> to PL on <CUR> Currency on Portfolio <PF> and Desk <DSK>\"\n",
    "    user_prompt_list = [\n",
    "    \"You are a financial assistant. Your task is to generate structured queries from given templates to create financial summaries.\",\n",
    "    \n",
    "    f\"Here is an example pattern for financial summaries: {refined_sample_summarized_pnl_commentaries[0]}.\",\n",
    "    \n",
    "    f\"You are provided with a list of rule-based templates in the format List[{title_comment_template}]. Extract meaningful queries from these templates.\",\n",
    "    \n",
    "    \"\"\"Generate at least 15 diverse queries that can be used to generate sample financial summaries.\n",
    "    \n",
    "    - The queries should focus on aggregations such as min, max, mean, and sum, or retrieve the top 5 / bottom 5 entities.  \n",
    "    - Avoid queries that fetch all rows or list all entities without aggregation.  \n",
    "    - Do not create separate queries for different aggregations on the same entity; instead, combine them into a single query.  \n",
    "    - Dont ask for a particular value; instead, ask for a top k or bottom k value. Say, top 5 Business Units or bottom 5 Desks.\n",
    "    - The queries should be sufficient to address the financial summary patterns mentioned above.  \n",
    "    - Replace all field values with placeholders using the format <FIELD>. Do not include actual values.  \n",
    "    - Do not summarize the data; just generate structured queries.\"\"\",\n",
    "    \n",
    "    \"You must use the tool `FinancialQueries`. Your response must strictly follow the argument structure of `FinancialQueries`.\"\n",
    "    ]\n",
    "\n",
    "    # Generate queries\n",
    "    queries = fin_qry_engine(user_prompt_list)[0]['queries']\n",
    "    LOGGER.info(f\"State: {state['state']} | Generated {len(queries)} queries\")\n",
    "\n",
    "    # Save the result to state var and set the cache flag to True\n",
    "    state['results'][state['state']] = queries\n",
    "    state['cache_flag'][state['state']] = True\n",
    "\n",
    "    # Assign id to each query with sha hash\n",
    "    for query in state['results'][state['state']]:\n",
    "        query['id'] = hashlib.sha256(json.dumps(query).encode()).hexdigest()\n",
    "\n",
    "    # Save the result to cache with the state name and {result} key\n",
    "    save_json_data(state['cache_location'][state['state']], {\"result\":state['results'][state['state']]})\n",
    "\n",
    "    LOGGER.info(f\"State: {state['state']} | Query generation completed, saved the result to cache and set the cache flag to True\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stat_query_agent(state: AgentState):\n",
    "    state['state'] = 'stat_query_generation'\n",
    "    LOGGER.info(f\"State: {state['state']} | Initializing the agent to refine old summaries\")\n",
    "\n",
    "    # Load the data from cache if the cache flag is set to True\n",
    "    cached_result = load_cached_results(state)\n",
    "    if cached_result:\n",
    "        return state\n",
    "    \n",
    "    ## It is not an AI task, it is a rule-based task. So, we can directly write the code here.\n",
    "\n",
    "    statistical_queries = [\n",
    "        \"What is average, max, min, varaince, sum of <NET> profit/loss?\",\n",
    "        \"What is average, max, min, varaince, sum of <FACTOR> profit/loss grouped by <BUIS>?\",\n",
    "        \"What is average, max, min, varaince, sum of <NET> profit/loss grouped by <CUR> currency?\",\n",
    "        \"What is average, max, min, varaince, sum of <NET> profit/loss grouped by top 5 <PF> portfolios?\",\n",
    "        \"What is average, max, min, varaince, sum of <NET> profit/loss grouped by bottom 5 <PF> portfolios?\",\n",
    "        \"What is average, max, min, varaince, sum of <NET> profit/loss grouped by top 5 <DSK> desks?\",\n",
    "        \"What is average, max, min, varaince, sum of <NET> profit/loss grouped by bottom 5 <DSK> desks?\",\n",
    "        \"What are the top currencies by average <NET> profit/loss?\",\n",
    "        \"What are the bottom currencies by average <NET> profit/loss?\",\n",
    "        \"What is the total count of transactions for each <FACTOR>?\",\n",
    "        \"What is the percentage contribution of each <FACTOR> to total profit/loss?\",\n",
    "        \"What is the trend of total <NET> profit/loss over time (daily, monthly, yearly)?\",\n",
    "        \"What is the moving average of <NET> profit/loss over the past 7 days?\",\n",
    "        \"What is the standard deviation of <NET> profit/loss grouped by <BUIS>?\",\n",
    "        \"What is the correlation between <FACTOR> and <NET> profit/loss?\",\n",
    "        \"What is the skewness and kurtosis of <NET> profit/loss distribution?\",\n",
    "        \"Which <PF> portfolios have the highest standard deviation in <NET> profit/loss?\",\n",
    "        \"Which <DSK> desks have the highest variance in <NET> profit/loss?\",\n",
    "        \"What is the probability distribution of <NET> profit/loss?\",\n",
    "        \"What is the cumulative sum of <NET> profit/loss over time?\",\n",
    "        \"Which <CUR> currency has the most volatile <NET> profit/loss?\",\n",
    "        \"What is the ratio of profitable to loss-making transactions per <FACTOR>?\",\n",
    "        \"Which <PF> portfolios have the most consistently positive (low variance) profits?\",\n",
    "        \"Which <FACTOR> contributes most to total profit/loss variance?\",\n",
    "        \"What is the ratio of profit to loss per <DSK> desk?\",\n",
    "        \"Which <CUR> currency contributes the most to total profit?\",\n",
    "        \"Which <FACTOR> has the highest frequency of losses?\"\n",
    "    ]\n",
    "    statistical_queries = [{\"query\": query} for query in statistical_queries]\n",
    "\n",
    "    # Save the result to state var and set the cache flag to True\n",
    "    state['results'][state['state']] = statistical_queries\n",
    "    state['cache_flag'][state['state']] = True\n",
    "\n",
    "    # Assign id to each query with sha hash\n",
    "    for query in state['results'][state['state']]:\n",
    "        query['id'] = hashlib.sha256(json.dumps(query).encode()).hexdigest()\n",
    "        \n",
    "    # Save the result to cache with the state name and {result} key\n",
    "    save_json_data(state['cache_location'][state['state']], {\"result\":state['results'][state['state']]})\n",
    "\n",
    "    LOGGER.info(f\"State: {state['state']} | Query generation completed, saved the result to cache and set the cache flag to True\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_data(state: AgentState):\n",
    "    state['state'] = 'register_data'\n",
    "    LOGGER.info(f\"State: {state['state']} | Initializing the agent to register data\")\n",
    "\n",
    "    # It is a very naive implementation, we can directly write the code here.\n",
    "    rule_based_title_comment_data  = load_json_data(state['cache_location']['rule_based_title_comment_data'])\n",
    "    \n",
    "    global TITLE_DATA_INMEM_DB\n",
    "    TITLE_DATA_INMEM_DB = get_title_data_inmemory_db(rule_based_title_comment_data)\n",
    "\n",
    "    LOGGER.info(f\"State: {state['state']} | Data registration completed, saved the data to in-memory DB. Global variable TITLE_DATA_INMEM_DB is set\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql_script_agent(state: AgentState):\n",
    "    state['state'] = 'sql_script_generation'\n",
    "    LOGGER.info(f\"State: {state['state']} | Initializing the agent to generate SQL script\")\n",
    "\n",
    "    # Load the data from cache if the cache flag is set to True\n",
    "    cached_result = load_cached_results(state)\n",
    "    if cached_result:\n",
    "        return state\n",
    "\n",
    "    # Initialize the model\n",
    "    sql_script_engine =  GeminiJsonEngine(\n",
    "                                    model_name=state['model']['model_name'],\n",
    "                                    basemodel=SQLScript,\n",
    "                                    temperature=state['model']['temperature'],\n",
    "                                    max_output_tokens=state['model']['max_output_tokens'],\n",
    "                                    systemInstructions=\"You are an expert financial bot. You will be given a table and you need to generate a SQL script to query the data from the table. \",\n",
    "                                    max_retries=state['model']['max_retries'],\n",
    "                                    wait_time=state['model']['wait_time'])\n",
    "\n",
    "    # Previous state outputs\n",
    "    subj_queries = state['results']['subj_query_generation']\n",
    "    stat_queries = state['results']['stat_query_generation']\n",
    "    all_queries = subj_queries + stat_queries\n",
    "\n",
    "    global TITLE_DATA_INMEM_DB\n",
    "    # Head of the table\n",
    "    _rule_based_title_comment_data_cols,_rule_based_title_comment_data_head = TITLE_DATA_INMEM_DB.query_data(\"SELECT * FROM title_data LIMIT 5\")\n",
    "    head = pd.DataFrame(_rule_based_title_comment_data_head, columns=_rule_based_title_comment_data_cols).drop(columns=['id','COMMENT']).head()\n",
    "\n",
    "    sql_scripts = []\n",
    "    for i, query in enumerate(all_queries):\n",
    "        user_sql_prompt = [\n",
    "            f\"You are a SQL expert. Your task is to write a SQL script to query data from the given table. Note: you are generating a SQL script for SQLLite's python library. You must be careful while writing complex queries as it is very sensitive.\",\n",
    "            f\"Library specific notes: STDDEV is not supported in SQLLite. You can use AVG and SUM to calculate the standard deviation.\",\n",
    "            f\"Here is the schema of the table `title_data`: {TITLE_DATA_INMEM_DB.metadata.tables}\",\n",
    "            f\"Here is the are the first few rows of the table `title_data`: {head}\",\n",
    "            f\"User is trying to answer the following query: {query['query']}\",\n",
    "            f\"Write a SQL script to answer the query using the tool `SQLScript`. Your answer must follow the argument strucure of the tool `SQLScript`. You are encouraged to use compound and complex SQL queries to answer the query.\"\n",
    "        ]\n",
    "        sql_script = sql_script_engine(user_sql_prompt)[0]\n",
    "        sql_scripts.append(sql_script)\n",
    "        LOGGER.info(f\"State: {state['state']} | {i}/{len(all_queries)} SQL script generated for the query: {query['query'][:20]} ... to {sql_script['sql_script'][:20]} ...\")\n",
    "\n",
    "    # Save the result to state var and set the cache flag to True\n",
    "    state['results'][state['state']] = sql_scripts\n",
    "    state['cache_flag'][state['state']] = True\n",
    "\n",
    "    # Assign id to each query with the same hash as the query\n",
    "    for raw_query, sql_script in zip(all_queries, state['results'][state['state']]):\n",
    "        sql_script['id'] = raw_query['id']\n",
    "\n",
    "    # Save the result to cache with the state name and {result} key\n",
    "    save_json_data(state['cache_location'][state['state']], {\"result\":state['results'][state['state']]})\n",
    "\n",
    "    LOGGER.info(f\"State: {state['state']} | SQL script generation completed, saved the result to cache and set the cache flag to True\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_result_agent(state:AgentState):\n",
    "    state['state'] = 'sql_result'\n",
    "    LOGGER.info(f\"State: {state['state']} | Initializing the agent to generate SQL result\")\n",
    "\n",
    "    # Load the data from cache if the cache flag is set to True\n",
    "    cached_result = load_cached_results(state)\n",
    "    if cached_result:\n",
    "        return state\n",
    "\n",
    "    global TITLE_DATA_INMEM_DB\n",
    "\n",
    "    # Previous state outputs\n",
    "    sql_scripts = state['results']['sql_script_generation']\n",
    "\n",
    "    # Execute the SQL scripts\n",
    "    sql_results = []\n",
    "    pass_count = 0\n",
    "    fail_count = 0\n",
    "    overlength_count = 0\n",
    "    for i, sql_script in enumerate(sql_scripts):\n",
    "        try:\n",
    "            columns, data = TITLE_DATA_INMEM_DB.query_data(sql_script['sql_script'])\n",
    "            sql_results.append({\n",
    "                \"id\": sql_script['id'],\n",
    "                \"columns\": columns,\n",
    "                \"data\": data,\n",
    "                \"status\": \"success\",\n",
    "                \"description\": sql_script['description'],\n",
    "                \"sql_script\": sql_script['sql_script']\n",
    "            })\n",
    "            if len(data) < 20:\n",
    "                LOGGER.info(f\"State: {state['state']} | {i}/{len(sql_scripts)} SQL script executed, {len(data)} rows returned\")\n",
    "                pass_count += 1\n",
    "            else:\n",
    "                LOGGER.warning(f\"State: {state['state']} | {i}/{len(sql_scripts)} SQL script executed, {len(data)} rows returned. Too many rows returned, consider refining the query\")\n",
    "                sql_results[-1]['status'] = \"overlength\"\n",
    "                overlength_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            LOGGER.error(f\"State: {state['state']} | {i}/{len(sql_scripts)} SQL script execution failed: {str(e)}. Skipping the query\")\n",
    "            sql_results.append({\n",
    "                \"id\": sql_script['id'],\n",
    "                \"columns\": [],\n",
    "                \"data\": [],\n",
    "                \"status\": \"failed\",\n",
    "                \"description\": sql_script['description'],\n",
    "                \"sql_script\": sql_script['sql_script']\n",
    "            })\n",
    "            fail_count += 1\n",
    "\n",
    "    LOGGER.info(f\"State: {state['state']} | SQL script execution completed, {pass_count} passed, {fail_count} failed, {overlength_count} overlength\")\n",
    "\n",
    "    # Save the result to state var and set the cache flag to True\n",
    "    state['results'][state['state']] = sql_results\n",
    "    state['cache_flag'][state['state']] = True\n",
    "\n",
    "    # Save the result to cache with the state name and {result} key\n",
    "    save_json_data(state['cache_location'][state['state']], {\"result\":state['results'][state['state']]})\n",
    "\n",
    "    LOGGER.info(f\"State: {state['state']} | SQL result generation completed, saved the result to cache and set the cache flag to True\")\n",
    "\n",
    "    return state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bucket_query_agent(state: AgentState):\n",
    "    state['state'] = 'bucket_query_generation'\n",
    "    LOGGER.info(f\"State: {state['state']} | Initializing the agent to generate bucket queries\")\n",
    "\n",
    "    # It is not an AI task, it is a rule-based task. So, we can directly write the code here.\n",
    "\n",
    "    # Load previous state outputs\n",
    "    old_summary = state['results']['refine_old_summaries'][0]\n",
    "    subj_results = state['results']['subj_query_generation']\n",
    "    stat_results = state['results']['stat_query_generation']\n",
    "    sql_results = [result for result in state['results']['sql_result'] if result['status'] == 'success']\n",
    "\n",
    "    # Subj IDs and  Stat IDs\n",
    "    subj_ids = [query['id'] for query in subj_results]\n",
    "    stat_ids = [query['id'] for query in stat_results]\n",
    "\n",
    "    # Filter SQL results with subj and stat IDs\n",
    "    sql_subj_results = [result for result in sql_results if result['id'] in subj_ids]\n",
    "    sql_stat_results = [result for result in sql_results if result['id'] in stat_ids]\n",
    "\n",
    "    # Bucket queries\n",
    "    sql_subj_result_qa_s = \"\\n\".join([f\"Q{i}. {result['description']}:\\n{pd.DataFrame(result['data'], columns=result['columns']).head()}\" for i, result in enumerate(sql_subj_results)])\n",
    "    sql_stat_result_qa_s = \"\\n\".join([f\"Q{i}. {result['description']}:\\n{pd.DataFrame(result['data'], columns=result['columns']).head()}\" for i, result in enumerate(sql_stat_results)])\n",
    "\n",
    "    \n",
    "    prompts = [[\n",
    "        f\"You are financial expert. Your task is to provide a DETAILED and SOPHISTICATED financial summary over P&L Trend and other financial metrics. You are provided with some structured questions and their answers. You need to generate the summary from the insights provided in the answers.\",\n",
    "        f\"The meaning for the columns are as follows: BUIS means Business Unit, DATE means Day of calcualtion, NET means Net Profit/Loss, Factor such as IRDelta, IRGamma, FXDelta etc., PROF_LOSS means Profit or Loss, CUR means Currency, PF means Portfolio, DSK means Desk.\",\n",
    "        f\"Here is a sample summary for reference (Just follow the pattern, not the exact values): {old_summary}\",\n",
    "        f\"Here are the structured questions and their answers: {qa}\",\n",
    "        f\"Generate a detailed financial summary based on the insights provided in the answers.\"\n",
    "    ] for qa in [sql_subj_result_qa_s, sql_stat_result_qa_s]]\n",
    "\n",
    "\n",
    "    # Update the state\n",
    "    state['results'][state['state']] = prompts\n",
    "\n",
    "    LOGGER.info(f\"State: {state['state']} | Bucket queries generated\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_result(state: AgentState):\n",
    "    state['state'] = 'final_result'\n",
    "    LOGGER.info(f\"State: {state['state']} | Initializing the agent to generate final result\")\n",
    "\n",
    "    # Load the data from cache if the cache flag is set to True\n",
    "    cached_result = load_cached_results(state)\n",
    "    if cached_result:\n",
    "        return state\n",
    "\n",
    "    # Initialize the model\n",
    "    gemini_simple_chat_engine = GeminiSimpleChatEngine(model_name=state['model']['model_name'], \n",
    "                                                   temperature=state['model']['temperature'],\n",
    "                                                   max_output_tokens=1024,\n",
    "                                                   systemInstructions=None,\n",
    "                                                   max_retries=state['model']['max_retries'],\n",
    "                                                   wait_time=state['model']['wait_time'])\n",
    "\n",
    "    # Previous state outputs\n",
    "    bucket_queries = state['results']['bucket_query_generation']\n",
    "\n",
    "    # Generate the final result\n",
    "    final_results = []\n",
    "    for i, bucket_query in enumerate(bucket_queries):\n",
    "        final_result = gemini_simple_chat_engine(bucket_query)\n",
    "        final_results.append(final_result)\n",
    "        LOGGER.info(f\"State: {state['state']} | {i}/{len(bucket_queries)} Final result generated from the bucket query. {final_result[:30]}...\")\n",
    "\n",
    "    # Save the result to state var and set the cache flag to True\n",
    "    state['results'][state['state']] = final_results\n",
    "    state['cache_flag'][state['state']] = True\n",
    "\n",
    "    # Save the result to cache with the state name and {result} key\n",
    "    save_json_data(state['cache_location'][state['state']], {\"result\":state['results'][state['state']]})\n",
    "\n",
    "\n",
    "    LOGGER.info(f\"State: {state['state']} | Final result generation completed, saved the result to cache and set the cache flag to True\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAgent:\n",
    "    def __init__(self, thread_id=None):\n",
    "        self.config = None\n",
    "        self.app = None\n",
    "        self.build(thread_id)\n",
    "\n",
    "    def build(self, thread_id):\n",
    "        workflow = StateGraph(AgentState)\n",
    "\n",
    "        # Nodes\n",
    "        workflow.add_node('start', start_agent)\n",
    "        workflow.add_node('end', end_agent)\n",
    "        workflow.add_node('refine_old_summaries', refine_old_summary_agent)\n",
    "        workflow.add_node('generate_subj_queries', generate_subj_query_agent)\n",
    "        workflow.add_node('generate_stat_queries', generate_stat_query_agent)\n",
    "        workflow.add_node('register_data', register_data)\n",
    "        workflow.add_node('sql_script_generation', generate_sql_script_agent)\n",
    "        workflow.add_node('sql_result_generation', sql_result_agent)\n",
    "        workflow.add_node('bucket_query_generation', generate_bucket_query_agent)\n",
    "        workflow.add_node('final_result', generate_final_result)\n",
    "\n",
    "        # Edges\n",
    "        workflow.add_edge('start', 'refine_old_summaries')\n",
    "        workflow.add_edge('refine_old_summaries', 'generate_subj_queries')\n",
    "        workflow.add_edge('generate_subj_queries', 'generate_stat_queries')\n",
    "        workflow.add_edge('generate_stat_queries', 'register_data')\n",
    "        workflow.add_edge('register_data', 'sql_script_generation')\n",
    "        workflow.add_edge('sql_script_generation', 'sql_result_generation')\n",
    "        workflow.add_edge('sql_result_generation', 'bucket_query_generation')\n",
    "        workflow.add_edge('bucket_query_generation', 'final_result')\n",
    "        workflow.add_edge('final_result', 'end')\n",
    "\n",
    "        # Compile\n",
    "        workflow.set_entry_point('start')\n",
    "        memory = MemorySaver()\n",
    "        self.app = workflow.compile(checkpointer=memory)\n",
    "        self.config = {\"configurable\":{\"thread_id\":str(thread_id)}}\n",
    "\n",
    "    def get_recent_state_snap(self):\n",
    "        snap = self.app.get_state(self.config).values.copy()\n",
    "        return snap\n",
    "    \n",
    "    def get_graph(self):\n",
    "        graph = self.app.get_graph(xray=True)\n",
    "        return graph\n",
    "    \n",
    "    def continue_flow(self, state):\n",
    "        self.app.invoke(state, config=self.config)\n",
    "        return self.get_recent_state_snap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_AGENT = MyAgent(thread_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MY_AGENT.get_recent_state_snap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       +-----------+         \n",
      "       | __start__ |         \n",
      "       +-----------+         \n",
      "              *              \n",
      "              *              \n",
      "              *              \n",
      "         +-------+           \n",
      "         | start |           \n",
      "         +-------+           \n",
      "              *              \n",
      "              *              \n",
      "              *              \n",
      "  +----------------------+   \n",
      "  | refine_old_summaries |   \n",
      "  +----------------------+   \n",
      "              *              \n",
      "              *              \n",
      "              *              \n",
      " +-----------------------+   \n",
      " | generate_subj_queries |   \n",
      " +-----------------------+   \n",
      "              *              \n",
      "              *              \n",
      "              *              \n",
      " +-----------------------+   \n",
      " | generate_stat_queries |   \n",
      " +-----------------------+   \n",
      "              *              \n",
      "              *              \n",
      "              *              \n",
      "     +---------------+       \n",
      "     | register_data |       \n",
      "     +---------------+       \n",
      "              *              \n",
      "              *              \n",
      "              *              \n",
      " +-----------------------+   \n",
      " | sql_script_generation |   \n",
      " +-----------------------+   \n",
      "              *              \n",
      "              *              \n",
      "              *              \n",
      " +-----------------------+   \n",
      " | sql_result_generation |   \n",
      " +-----------------------+   \n",
      "              *              \n",
      "              *              \n",
      "              *              \n",
      "+-------------------------+  \n",
      "| bucket_query_generation |  \n",
      "+-------------------------+  \n",
      "              *              \n",
      "              *              \n",
      "              *              \n",
      "      +--------------+       \n",
      "      | final_result |       \n",
      "      +--------------+       \n",
      "              *              \n",
      "              *              \n",
      "              *              \n",
      "          +-----+            \n",
      "          | end |            \n",
      "          +-----+            \n"
     ]
    }
   ],
   "source": [
    "graph = MY_AGENT.get_graph()\n",
    "print(graph.draw_ascii())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m2025-03-10 02:20:57,638 - INFO ==> Starting the agent-assist\u001b[0m\n",
      "\u001b[1;32m2025-03-10 02:20:57,640 - INFO ==> State: refine_old_summaries | Initializing the agent to refine old summaries\u001b[0m\n",
      "\u001b[1;32m2025-03-10 02:20:57,640 - INFO ==> State: refine_old_summaries | Loaded cached data and skipping the model, 3 old result found\u001b[0m\n",
      "\u001b[1;32m2025-03-10 02:20:57,641 - INFO ==> State: subj_query_generation | Initializing the agent to refine old summaries\u001b[0m\n",
      "\u001b[1;32m2025-03-10 02:20:57,641 - INFO ==> State: subj_query_generation | Loaded cached data and skipping the model, 15 old result found\u001b[0m\n",
      "\u001b[1;32m2025-03-10 02:20:57,642 - INFO ==> State: stat_query_generation | Initializing the agent to refine old summaries\u001b[0m\n",
      "\u001b[1;32m2025-03-10 02:20:57,643 - INFO ==> State: stat_query_generation | Loaded cached data and skipping the model, 27 old result found\u001b[0m\n",
      "\u001b[1;32m2025-03-10 02:20:57,644 - INFO ==> State: register_data | Initializing the agent to register data\u001b[0m\n",
      "\u001b[1;32m2025-03-10 02:20:57,756 - INFO ==> State: register_data | Data registration completed, saved the data to in-memory DB. Global variable TITLE_DATA_INMEM_DB is set\u001b[0m\n",
      "\u001b[1;32m2025-03-10 02:20:57,757 - INFO ==> State: sql_script_generation | Initializing the agent to generate SQL script\u001b[0m\n",
      "\u001b[1;32m2025-03-10 02:20:57,757 - INFO ==> State: sql_script_generation | Loaded cached data and skipping the model, 42 old result found\u001b[0m\n",
      "\u001b[1;32m2025-03-10 02:20:57,758 - INFO ==> State: sql_result | Initializing the agent to generate SQL result\u001b[0m\n",
      "\u001b[1;32m2025-03-10 02:20:57,760 - INFO ==> State: sql_result | Loaded cached data and skipping the model, 42 old result found\u001b[0m\n",
      "\u001b[1;32m2025-03-10 02:20:57,761 - INFO ==> State: bucket_query_generation | Initializing the agent to generate bucket queries\u001b[0m\n",
      "\u001b[1;32m2025-03-10 02:20:57,775 - INFO ==> State: bucket_query_generation | Bucket queries generated\u001b[0m\n",
      "\u001b[1;32m2025-03-10 02:20:57,776 - INFO ==> State: final_result | Initializing the agent to generate final result\u001b[0m\n",
      "\u001b[1;32m2025-03-10 02:20:57,778 - INFO ==> State: final_result | Loaded cached data and skipping the model, 2 old result found\u001b[0m\n",
      "\u001b[1;32m2025-03-10 02:20:57,778 - INFO ==> Ending the agent-assist\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "MY_AGENT.continue_flow({\n",
    "    'state': 'start',\n",
    "    'model':{\n",
    "        'model_name':'gemini-2.0-flash-001',\n",
    "        'temperature': 0.5,\n",
    "        'max_output_tokens': 512,\n",
    "        'max_retries':5,\n",
    "        'wait_time':30\n",
    "    },\n",
    "    'results':{\n",
    "        'refine_old_summaries':[],\n",
    "        'subj_query_generation':[],\n",
    "        'stat_query_generation':[],\n",
    "        'sql_script_generation':[],\n",
    "        'sql_result':[],\n",
    "        'bucket_query_generation':[],\n",
    "        'final_result':[]\n",
    "    },\n",
    "    'cache_location':{\n",
    "        \"sample_summarized_pnl_commentaries\":\"../sample_data/sample_summarized_pnl_commentaries.json\",\n",
    "        \"rule_based_title_comment_data\":\"../sample_data/rule_based_title_comment_data.json\",\n",
    "        \n",
    "        \"refine_old_summaries\":\"../sample_data/cached/refine_old_summaries.json\",\n",
    "        \"subj_query_generation\":\"../sample_data/cached/subj_query_generation.json\",\n",
    "        \"stat_query_generation\":\"../sample_data/cached/stat_query_generation.json\",\n",
    "        \"sql_script_generation\":\"../sample_data/cached/sql_script_generation.json\",\n",
    "        \"sql_result\":\"../sample_data/cached/sql_result.json\",\n",
    "        \"final_result\":\"../sample_data/cached/final_result.json\"\n",
    "    },\n",
    "    'cache_flag':{\n",
    "        \"refine_old_summaries\":True,\n",
    "        \"subj_query_generation\":True,\n",
    "        \"stat_query_generation\":True,\n",
    "        \"sql_script_generation\":True,\n",
    "        \"sql_result\":True,\n",
    "        \"final_result\":True\n",
    "    }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "snap = MY_AGENT.get_recent_state_snap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall, the top 5 factors impacting net profit/loss were Credit Spread (+€204m), Bond Basis (+€200m), IRGamma (+€193m), Theta (+€184m), and FXDelta (+€171m). However, specific desks, particularly US/LDN and LATAM/NYC, experienced the lowest net profit/loss.\n",
      "\n",
      "Analyzing portfolio performance, the American London CEEMAEA Portfolio stands out with the highest overall net profit (+€392m), followed by LATAM Emerging (+€380m) and European CEEMAEA (+€346m). Within the American London CEEMAEA Portfolio, Bond Basis contributed +€78m, Credit Spread +€61m, FXDelta +€58m, IRDelta +€63m, and IRGamma +€67m.\n",
      "\n",
      "Business unit performance, broken down by currency, reveals that CEEMAEA's net profit was significantly influenced by INR (+€94m), MXN (+€83m), CZK (+€82m), EUR (+€79m), and BRL (+€75m). However, some CEEMAEA and LATAM business units experienced the lowest net profit/loss overall.\n",
      "\n",
      "Desk-level analysis shows that Bond Basis had a significant impact on LATAM/NYC DSK (+€81m), while Credit Spread and IRGamma were key drivers for EU/LDN DSK (+€76m and +€74m respectively). Credit Spread also had a substantial impact on US/LDN DSK (+€68m), and Bond Basis contributed significantly to EU/LDN DSK (+€66m).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(snap['results']['final_result'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average Net Profit/Loss (NET) across all transactions is approximately €1.01 million, with a total sum of €1.12 billion across 1103 transactions. The skewness of 0.015 and kurtosis of -1.29 indicate a near-normal distribution with slightly flatter tails.\n",
      "\n",
      "**Business Unit (BUIS) Performance:** Both CEEMAEA and LATAM Business Units show an equal distribution of profit and loss transactions, with a sum of €0.\n",
      "\n",
      "**Currency (CUR) Performance:** The top-performing currencies by average NET are INR (€1.05m), GBP (€1.05m), and EUR (€1.04m). The bottom-performing currencies are BRL (€0.90m) and USD (€0.97m). USD has the highest total profit with €0.\n",
      "\n",
      "**Portfolio (PF) Performance:** Among the bottom three portfolios by average NET, American London CEEMAEA Portfolio has the highest average NET at €1.06m, followed by LATAM Emerging Portfolio (€1.03m) and European CEEMAEA Portfolio (€0.95m). The portfolios with the highest standard deviation in NET are European CEEMAEA Portfolio, American London CEEMAEA Portfolio, and LATAM Emerging Portfolio. All portfolios have an average profit of €0.\n",
      "\n",
      "**Desk (DSK) Performance:** The top three desks by average NET are US/LDN DSK (€1.03m), LATAM/NYC DSK (€1.02m), and EU/LDN DSK (€0.99m). EU/LDN DSK shows the highest variance in NET. EU/LDN DSK has the highest profit-to-loss ratio (1.12), followed by US/LDN DSK (1.05), while LATAM/NYC DSK has a lower ratio (0.80).\n",
      "\n",
      "**Factor Analysis:** BondBasis, CreditSpread, IRDelta, IRGamma, and FXDelta are the most frequent factors. IRGamma has the highest profit-to-loss ratio (1.02), while IRDelta has the lowest (0.86). BondBasis has the highest frequency of losses.\n",
      "\n",
      "**Volatility and Risk:** The standard deviation of NET profit/loss is significant for both CEEMAEA and LATAM Business Units, indicating high volatility.\n",
      "\n",
      "**Overall:** The overall performance shows a positive average NET profit/loss, but with significant variations across currencies, portfolios, and desks. Risk management should focus on the high volatility observed in CEEMAEA and LATAM Business Units and the factors contributing to losses, particularly BondBasis.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(snap['results']['final_result'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
